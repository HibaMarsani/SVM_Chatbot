{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb7d7ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\omen\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\omen\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: click in c:\\users\\omen\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\omen\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\omen\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\omen\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "254064ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2180ff23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\OMEN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29651625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f215084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to {self.name}!\n",
      "You can ask me questions about your dataset and I will do my best to assist you.\n",
      "Enter 'quit' to exit the chat.\n",
      "User: hi\n",
      "Chatbot: Hi there!\n",
      "User: load dataset\n",
      "Chatbot: Please provide the file path of the dataset on your computer: C:\\Users\\OMEN\\anaconda3\\diabetes.csv\n",
      "Chatbot: Dataset loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class SVMBot:\n",
    "    def __init__(self, name):\n",
    "        self.dataset = None\n",
    "        self.model = None\n",
    "        self.vectorizer = None\n",
    "        self.name = name\n",
    "\n",
    "    def read_dataset(self, dataset_link):\n",
    "        try:\n",
    "            self.dataset = pd.read_csv(dataset_link)\n",
    "        except:\n",
    "            raise ValueError(\"Invalid dataset link. Please try again.\")\n",
    "\n",
    "    def get_column_names(self):\n",
    "        if self.dataset is None:\n",
    "            raise ValueError(\"Dataset not loaded. Please load dataset first.\")\n",
    "        return self.dataset.columns.tolist()\n",
    "\n",
    "    def train_model(self, X_column, Y_column):\n",
    "        if self.dataset is None:\n",
    "            raise ValueError(\"Dataset not loaded. Please load dataset first.\")\n",
    "\n",
    "        if X_column not in self.dataset.columns.tolist() or Y_column not in self.dataset.columns.tolist():\n",
    "            raise ValueError(\"X or Y column not found in the dataset. Please provide valid column names.\")\n",
    "\n",
    "        X = self.dataset[X_column].values.reshape(-1, 1)\n",
    "        Y = self.dataset[Y_column].values\n",
    "\n",
    "        # Perform feature scaling on X values\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "       # Train the SVM model\n",
    "        self.model = SVR(kernel='rbf')\n",
    "        self.model.fit(X_scaled, Y)\n",
    "\n",
    "    def predict(self, X_value):\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Please train model first.\")\n",
    "\n",
    "        # Reshape X_value into a 2D array\n",
    "        X_value = np.array(X_value).reshape(1, -1)\n",
    "\n",
    "        Y_pred = self.model.predict(X_value)\n",
    "\n",
    "        return Y_pred[0]\n",
    "\n",
    "    def process_text(self, text):\n",
    "        # Tokenize text\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # Remove stop words\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "        # Perform stemming\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def chat(self):\n",
    "        print( \"Welcome to {self.name}!\")\n",
    "        print(\"You can ask me questions about your dataset and I will do my best to assist you.\")\n",
    "        print(\"Enter 'quit' to exit the chat.\")\n",
    "\n",
    "        while True:\n",
    "            user_input = input(\"User: \")\n",
    "            user_input = user_input.lower()\n",
    "\n",
    "            if user_input == 'quit':\n",
    "                print(\"Chatbot: Goodbye!\")\n",
    "                break\n",
    "\n",
    "            # Process user input\n",
    "            tokens = self.process_text(user_input)\n",
    "\n",
    "            # Check for keywords and respond accordingly\n",
    "\n",
    "            if 'hello' in tokens or 'hi' in tokens:\n",
    "                print(\"Chatbot: Hi there!\")\n",
    "            elif 'how are you' in user_input:\n",
    "                print(\"Chatbot: I'm doing well, thank you!\")\n",
    "            elif 'thank you' in tokens or 'thanks' in tokens:\n",
    "                print(\"Chatbot: You're welcome!\")\n",
    "            elif 'bye' in tokens or 'goodbye' in tokens:\n",
    "                print(\"Chatbot: Goodbye!\") \n",
    "                break\n",
    "\n",
    "            elif 'load' in tokens and 'dataset' in tokens:\n",
    "                while True:\n",
    "                    try:\n",
    "                        file_path = input(\"Chatbot: Please provide the file path of the dataset on your computer: \")\n",
    "                        self.read_dataset(file_path)\n",
    "                        print(\"Chatbot: Dataset loaded successfully!\")\n",
    "                        break\n",
    "                    except ValueError as e:\n",
    "                        print(\"Chatbot: Failed to load dataset. Please try again.\")\n",
    "          \n",
    "            elif 'train' in tokens and 'model' in tokens:\n",
    "                while True:\n",
    "                    try:\n",
    "                        X_column = input(\"Chatbot: Please enter the name of the X column for training the model: \")\n",
    "                        Y_column = input(\"Chatbot: Please enter the name of the Y column for training the model: \")\n",
    "                        self.train_model(X_column, Y_column)\n",
    "                        print(\"Chatbot: Model trained successfully!\")\n",
    "                        break\n",
    "                    except ValueError as e:\n",
    "                        print(\"Chatbot: Failed to train model. \" + str(e))\n",
    "\n",
    "            elif 'get' in tokens or 'column names' in tokens:\n",
    "                print(self.dataset.columns)\n",
    "\n",
    "\n",
    "            elif 'predict' in tokens:\n",
    "                if self.model is None:\n",
    "                    print(\"Chatbot: Model not trained. Please train model first.\")\n",
    "                else:\n",
    "                    try:\n",
    "                        X_value = float(input(\"Chatbot: Please enter the value of X for prediction: \"))\n",
    "                        Y_pred = self.predict(X_value)\n",
    "                        print(\"Chatbot: The predicted value of Y is: \" + str(Y_pred))\n",
    "                    except ValueError:\n",
    "                        print(\"Chatbot: Invalid input. Please enter a numerical value for X.\")\n",
    "\n",
    "            elif 'process' in tokens and 'text' in tokens:\n",
    "                text = input(\"Chatbot: Please enter the text for processing: \")\n",
    "                processed_text = self.process_text(text)\n",
    "                print(\"Chatbot: The processed text is: \")\n",
    "                print(processed_text)\n",
    "\n",
    "            else:\n",
    "                print(\"Chatbot: I'm sorry, I don't understand. Can you please rephrase your question?\")\n",
    "bot = SVMBot(\"Support Vector Machine Bot\")\n",
    "bot.chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf32b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
